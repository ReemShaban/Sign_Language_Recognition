{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Recognition - Second Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Importing os to make it easier to work with file paths\n",
    "import os  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Folders to Store Collected Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentDirectory = os.getcwd()\n",
    "\n",
    "IMAGES_PATH = os.path.join(currentDirectory , 'Sign_Training_Images')\n",
    "\n",
    "signs = np.array(['Peace', 'I Love You', 'Good', 'House'])\n",
    "\n",
    "# Note that the number of frames specified here, however only 300 will be used for training and testing \n",
    "# and the first 50 will be disregarded. (As during the first few frames the hand movement is still getting adjusted and are not an accurate representation of the sign)\n",
    "framesPerSign = 350\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sign in signs:\n",
    "    try:\n",
    "        os.makedirs(os.path.join(IMAGES_PATH, sign))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "background = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(image, weight):\n",
    "    \n",
    "    global background\n",
    "\n",
    "    if background is None:\n",
    "        background = image.copy().astype(\"float\")\n",
    "        return\n",
    "\n",
    "\n",
    "    cv2.accumulateWeighted(image, background, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HandSegmentation(image, threshold = 25):\n",
    "    \n",
    "    global background\n",
    "\n",
    "    differenceImage = cv2.absdiff(background.astype(\"uint8\"), image)\n",
    "\n",
    "\n",
    "    thresholdImage = cv2.threshold(differenceImage, threshold, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "\n",
    "    contours, _ = cv2.findContours(thresholdImage.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "    if len(contours) == 0:\n",
    "        \n",
    "        return\n",
    "    \n",
    "    else:\n",
    "\n",
    "        segmentedImage = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        return (thresholdImage, segmentedImage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Sign Images for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectTrainingData(sign):\n",
    "    \n",
    "    global IMAGES_PATH\n",
    "    global framesPerSign\n",
    "    \n",
    "    weight = 0.5\n",
    "    \n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    ROItop, ROIright, ROIbottom, ROIleft = 20, 460, 260, 725\n",
    "\n",
    "    frameNumber = 0\n",
    "    \n",
    "    backgroundFrames = 100\n",
    "\n",
    "    while(frameNumber < backgroundFrames + framesPerSign):\n",
    "\n",
    "        \n",
    "        ret, frame = camera.read()\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        \n",
    "        regionOfInterest = frame[ROItop : ROIbottom, ROIright : ROIleft]\n",
    "\n",
    "\n",
    "        grayscaleImage = cv2.cvtColor(regionOfInterest, cv2.COLOR_BGR2GRAY)\n",
    "        grayscaleImage = cv2.GaussianBlur(grayscaleImage, (7, 7), 0)\n",
    "\n",
    "\n",
    "        if frameNumber < backgroundFrames:\n",
    "            \n",
    "            running_average(grayscaleImage, weight)\n",
    "            cv2.putText(frame, \"Collecting Background...\",(160, 320), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        \n",
    "        else:\n",
    "\n",
    "            handRegion = HandSegmentation(grayscaleImage)\n",
    "            \n",
    "            if handRegion is not None:\n",
    "                \n",
    "                thresholdImage, segmentedImage = handRegion\n",
    "\n",
    "                \n",
    "                cv2.drawContours(frame, [segmentedImage + (ROIright, ROItop)], -1, (0, 0, 255))                    \n",
    "\n",
    "                cv2.putText(frame, 'Collecting Frames for {} Image Number {}'.format(sign,frameNumber - backgroundFrames), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)  \n",
    "                cv2.imshow(\"Theshold Image\", thresholdImage)\n",
    "                cv2.imwrite(IMAGES_PATH + '/' + sign + '/'+ str(frameNumber - backgroundFrames)+'.jpg', thresholdImage)\n",
    "\n",
    "        \n",
    "        cv2.rectangle(frame, (ROIleft, ROItop), (ROIright, ROIbottom), (0,255,0), 2)\n",
    "\n",
    "        \n",
    "        frameNumber += 1\n",
    "\n",
    "\n",
    "            \n",
    "        cv2.imshow(\"Sign Language Recognition\", frame)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "   \n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectTrainingData('Peace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectTrainingData('I Love You')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectTrainingData('Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "collectTrainingData('House')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building and Training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "signImages = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 960 images belonging to 4 classes.\n",
      "Found 240 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "trainingData = signImages.flow_from_directory(directory=IMAGES_PATH, target_size=(64,64),subset=\"training\", class_mode='categorical', batch_size=10,shuffle=True)\n",
    "testingData = signImages.flow_from_directory(directory=IMAGES_PATH, target_size=(64,64), subset=\"validation\",class_mode='categorical', batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "\n",
    "model.add(Dense(4,activation =\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduceLearningRate = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 11s 103ms/step - loss: 6.6250 - accuracy: 0.7082 - val_loss: 0.8947 - val_accuracy: 0.7875\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 10s 107ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.8239 - val_accuracy: 0.8042\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 16s 168ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.8190 - val_accuracy: 0.8042\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 14s 149ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8221 - val_accuracy: 0.8042\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 12s 121ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8217 - val_accuracy: 0.8042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd202352e10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainingData, epochs=10, callbacks=[reduceLearningRate, earlyStopping],  validation_data = testingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 413,572\n",
      "Trainable params: 413,572\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving / Loading model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Second_Sign_Language_Model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Second_Sign_Language_Model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Model Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Model has a Loss of 0.0015983032062649727 and an Accuracy of 100.0 %\n"
     ]
    }
   ],
   "source": [
    "testImages, testImageslabels = next(testingData)\n",
    "\n",
    "modelEvaluationScore = model.evaluate(testImages, testImageslabels, verbose=0)\n",
    "\n",
    "\n",
    "print(f' The Model has a Loss of {modelEvaluationScore[0]} and an Accuracy of {modelEvaluationScore[1] * 100} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:'Good',1:'House',2:'I Love You',3:'Peace'}\n",
    "\n",
    "predictions = model.predict(testImages, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Labels\n",
      "--------------------------------\n",
      "House  \n",
      "I Love You  \n",
      "Peace  \n",
      "I Love You  \n",
      "I Love You  \n",
      "House  \n",
      "House  \n",
      "I Love You  \n",
      "I Love You  \n",
      "House  \n",
      "\n",
      "\n",
      "prediction Labels\n",
      "--------------------------------\n",
      "House  \n",
      "I Love You  \n",
      "Peace  \n",
      "I Love You  \n",
      "I Love You  \n",
      "House  \n",
      "House  \n",
      "I Love You  \n",
      "I Love You  \n",
      "House  \n"
     ]
    }
   ],
   "source": [
    "print('Actual Labels')\n",
    "print('--------------------------------')\n",
    "\n",
    "for label in testImageslabels :\n",
    "    print(f'{label_map[np.argmax(label)]}  ')\n",
    "\n",
    "    \n",
    "print('\\n')    \n",
    "print(\"prediction Labels\")\n",
    "print('--------------------------------')\n",
    "\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(f'{label_map[np.argmax(i)]}  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 0.5\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "ROItop, ROIright, ROIbottom, ROIleft = 20, 460, 260, 725\n",
    "\n",
    "frameNumber = 0\n",
    "\n",
    "backgroundFrames = 100\n",
    "\n",
    "while(True):\n",
    "\n",
    "        \n",
    "    ret, frame = camera.read()\n",
    "\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "        \n",
    "    regionOfInterest = frame[ROItop : ROIbottom, ROIright : ROIleft]\n",
    "\n",
    "\n",
    "    grayscaleImage = cv2.cvtColor(regionOfInterest, cv2.COLOR_BGR2GRAY)\n",
    "    grayscaleImage = cv2.GaussianBlur(grayscaleImage, (7, 7), 0)\n",
    "\n",
    "\n",
    "    if frameNumber < backgroundFrames:\n",
    "            \n",
    "        running_average(grayscaleImage, weight)\n",
    "        cv2.putText(frame, \"Collecting Background...\", (160, 320), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        handRegion = HandSegmentation(grayscaleImage)\n",
    "            \n",
    "        if handRegion is not None:\n",
    "                \n",
    "            thresholdImage, segmentedImage = handRegion\n",
    "\n",
    "                \n",
    "            cv2.drawContours(frame, [segmentedImage + (ROIright, ROItop)], -1, (0, 0, 255))                    \n",
    "\n",
    "            \n",
    "            cv2.imshow(\"Theshold Image\", thresholdImage)\n",
    "            \n",
    "            thresholdImage = cv2.resize(thresholdImage, (64, 64))\n",
    "            thresholdImage = cv2.cvtColor(thresholdImage, cv2.COLOR_GRAY2RGB)\n",
    "            thresholdImage = np.reshape(thresholdImage, (1,thresholdImage.shape[0],thresholdImage.shape[1],3))\n",
    "            \n",
    "            \n",
    "            prediction = model.predict(thresholdImage)\n",
    "            \n",
    "            cv2.putText(frame, label_map[np.argmax(prediction)], (180, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        \n",
    "    cv2.rectangle(frame, (ROIleft, ROItop), (ROIright, ROIbottom), (0,255,0), 2)\n",
    "\n",
    "        \n",
    "    frameNumber+= 1\n",
    "\n",
    "\n",
    "            \n",
    "    cv2.imshow(\"Sign Language Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
