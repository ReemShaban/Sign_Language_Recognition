{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In step 1. we are going to make sure we installed all the nesseray libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.4.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: opencv-python in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (4.5.2.54)\n",
      "Requirement already satisfied: mediapipe in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (0.8.5)\n",
      "Requirement already satisfied: sklearn in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: matplotlib in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.15.0)\n",
      "Requirement already satisfied: numpy~=1.19.2 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.19.5)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.12.1)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.36.2)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (2.10.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.13.0)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.12)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (3.17.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (2.4.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (0.3.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.1.2)\n",
      "Requirement already satisfied: grpcio~=1.32.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: tensorboard~=2.4 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.4.1) (2.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.4.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (1.32.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.1) (41.4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (0.23)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.1) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.1) (2019.9.11)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.1) (3.1.1)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from mediapipe) (4.5.2.54)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from mediapipe) (19.2.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from sklearn) (0.21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.1) (7.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/reemo/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.4.1 opencv-python mediapipe sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In step 2. we're going to make sure that we can access our webcam using opencv and then what we're going to apply a secondary layer in which we're going to make detections using mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up media pipe holistics\n",
    "\n",
    "mp_holistic = mp.solutions.holistic #bringing in a holistic model to make our detections\n",
    "mp_drawing = mp.solutions.drawing_utils #our drawing utilities to draw the previous made detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# media pip holistic detection function\n",
    "# first grab the image we convert it from bgr to rgb then set it to unwritable so this saves a little bit of memory\n",
    "# then we make our detection convert it or set it back to writable \n",
    "# then convert it from rgb to bgr so by default when we get a feed from opencv\n",
    "# it reads that feed in the channel format of bgr so blue green red \n",
    "# but when we actually go to make a detection using mediapipe we need it to be in the format of rgb \n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image= cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #color conversion from bgr to rgb\n",
    "    image.flags.writeable = False #set our image writable status to false \n",
    "    results = model.process(image) #detecting using media pip to make predictions\n",
    "    image.flags.writeable = True #image is now writable again\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) #color conversion from rgb to bgr\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we're going to grab the results from the detecting model \n",
    "#and render them onto the image so we can actually see our different landmarks\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    #helper function that comes with mediapipe that makes it easier to draw landmarks onto an image \n",
    "    #mp_holistic shows what landmark is connected to what other landmarks\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)#draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)#draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)#draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)#draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as the drawing function but with some updates on the formatting of the landmarks\n",
    "#specifiying the colors,thickniss and radious\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (80, 110, 10), thickness=1, circle_radius=1),\n",
    "                              mp_drawing.DrawingSpec(color= (80, 256, 121), thickness=1, circle_radius=1),\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (80, 22, 10), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color= (80, 44, 121), thickness=2, circle_radius=2),                              \n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (121, 22, 76), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color= (121, 44, 250), thickness=2, circle_radius=2),                              \n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color= (245, 117, 66), thickness=2, circle_radius=4),\n",
    "                              mp_drawing.DrawingSpec(color= (245, 66, 230), thickness=2, circle_radius=2),                              \n",
    "                             )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
     ]
    }
   ],
   "source": [
    "#we're doing to set up a video capture then loop through every single frame and render it to the screen \n",
    "#so the looping frames is going to look like a video as the basic idea of a video is, just multiple frames stacked together \n",
    "\n",
    "cap = cv2.VideoCapture(0) #accessing our webcam\n",
    "\n",
    "#with statement to be able to access media pip holistic model\n",
    "#the model will make an initial detection and then from there it'll track the key points\n",
    "                           #our intial detection          #tracking confidance\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened(): #double checking that we're still accessing our webcam\n",
    "\n",
    "        ret, frame = cap.read() #reading the feed of the webcam\n",
    "\n",
    "        image, results= mediapipe_detection(frame, holistic) #make prediction using media pip\n",
    "        print(results)\n",
    "        \n",
    "        draw_styled_landmarks(image, results)#draw styled landmarks with different colors function above\n",
    "\n",
    "        cv2.imshow('Project Feed', image) #show to the frame (image) to the screen\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'): #breaking the loop by waiting for a key ('q') to be pressed\n",
    "            break\n",
    "\n",
    "    cap.release() #release our webcam\n",
    "    cv2.destroyAllWindows()#closing down our frame\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_landmarks(frame, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In step 3. extracting the keypoint values into a format that we're able to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    #convert it all into one big array to be in this particular format when pass it to our lstm model\n",
    "    pose= np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    #it is going to give an error if we don't have our hand in the frame so we are going to replace the blank array with 0s one \n",
    "    #so if we have results it will extrct those values else it will replace it with zero array\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    #oncatenate pose face left hand and right hand to use those key points to actually do our sign language detection\n",
    "    return np.concatenate([pose, face, lh, rh])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. setting up our folders for our array collection so to output a result of these key points so our key points are effectively going to form our frame values to use those extracted key points to go and decode our sign language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('Signs_Data') #path for the exported data(numpy arrays)\n",
    "\n",
    "actions = np.array(['Hello', 'Goodbye', 'Nice To Meet You', 'Thanks', 'House'])#actions that we try to detect\n",
    "\n",
    "no_sequences = 50 #collecting 50 videos of data for each action\n",
    "\n",
    "sequence_length = 25 #25 frames,25 different sets of key points to be able to classify that action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store our data\n",
    "#create one folder for each action\n",
    "#and within each folder we are going to have a folder for each sequance of action so 50 folders\n",
    "\n",
    "#loop through all of our different actions\n",
    "for action in actions:\n",
    "    #loop through the 50 different videos that we're going to be collecting or 50 different frame sets\n",
    "    for sequence in range(no_sequences):\n",
    "        try:\n",
    "            #create a new folder called mp_data and then it's going to create a sub folder per action\n",
    "            #then it'll create a sequence folder\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. it will take a snapshot at each point in time so we're going to loop through each one of our actions and collect our actions and then loop through and collect a set of frames per video (25 frames per video) then collect 50 videos and we're going to do that three times for each action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### it will say starting collection we'll then get two seconds to get into position then perform our action for 30 frames it'll then go to starting collection again so we're going to do that 30 times per action and that will give us 30 frames for 30 sequences for each individual of our three different actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    #loop through our actions\n",
    "    for action in actions:\n",
    "        #loop through our sequences(videos:50)\n",
    "        for sequence in range(no_sequences):\n",
    "             #loop through our sequences length(video length:25)\n",
    "            for frame_num in range(sequence_length):\n",
    "                #read the feed\n",
    "                ret, frame = cap.read()\n",
    "                #make detections\n",
    "                image, results= mediapipe_detection(frame, holistic)\n",
    "\n",
    "                #draw formatted landmarks\n",
    "                draw_styled_landmarks(image, results)\n",
    "                #for each video if we're at frame 0 we're going to take a break(2000: 2 sec)\n",
    "                if frame_num == 0:\n",
    "                    #outputting text to our screen\n",
    "                    #print started collecting\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120,200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4, cv2.LINE_AA)\n",
    "                    #print the action we are collecting for and the number of video we are at\n",
    "                    cv2.putText(image, 'Collecting Frames for {} Video Number {}'.format(action, sequence), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)  \n",
    "                    cv2.imshow('Project Feed', image)\n",
    "                    #break at 2secs\n",
    "                    cv2.waitKey(2000)\n",
    "                    \n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting Frames for {} Video Number {}'.format(action, sequence), (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)  \n",
    "                    cv2.imshow('Project Feed', image)\n",
    "                \n",
    "                #apply keypoint extraction    \n",
    "                Keypoints = extract_keypoints(results)\n",
    "                #save the keypoints values to the folders\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                np.save(npy_path, Keypoints)\n",
    "                #exit when ('q') is pressed\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "    #release the webcame\n",
    "    cap.release()\n",
    "    #close the frame window\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6. import train test split from scikit learn to allow us to create a training and a testing sets then we're going to import the two categorical function from keras utilities to help us with our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a label array or a label dictionary to represent each one of our different actions\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 0, 'Goodbye': 1, 'Nice To Meet You': 2, 'Thanks': 3, 'House': 4}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating two blank arrays for sequences and labels\n",
    "#sequences is going to represent our feature data or our x data\n",
    "#labels is effectively going to represent our labels or our y data\n",
    "#use our features and train a model to represent the relationship between our labels\n",
    "sequences, labels = [], []\n",
    "#loop through each of our actions\n",
    "for action in actions:\n",
    "    #loop through each of our sequences(videos:50)\n",
    "    for sequence in range(no_sequences):\n",
    "        #blank array to represent all of the different frames that we got for that particular sequence\n",
    "        window = []\n",
    "        #loop through each one of the frames(25 frames length)\n",
    "        for frame_num in range(sequence_length):\n",
    "            #load up that frame and add it to the window\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "            \n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 25, 1662)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 25, 1662)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7. training our lstm neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard callback is a web app that's offered as part of the tensorflow package\n",
    "#that allows you to monitor your neural network training and accuracy\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neural network set up\n",
    "\n",
    "#first instantiating the model \n",
    "#using Sequential makes it easy to build up your model\n",
    "model = Sequential()\n",
    "\n",
    "#adding three sets of lstm layers\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(25,1662)))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "\n",
    "#adding three sets of dense layers\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(actions.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics= ['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 8s 221ms/step - loss: 1.6544 - categorical_accuracy: 0.2317\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 1s 154ms/step - loss: 1.8872 - categorical_accuracy: 0.2552\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 1s 145ms/step - loss: 1.5809 - categorical_accuracy: 0.2743\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 2s 204ms/step - loss: 1.5961 - categorical_accuracy: 0.2442\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 1s 171ms/step - loss: 2.0292 - categorical_accuracy: 0.1919\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 1.5622 - categorical_accuracy: 0.2023\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 1s 139ms/step - loss: 1.5037 - categorical_accuracy: 0.3050\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 1s 138ms/step - loss: 1.4736 - categorical_accuracy: 0.3331\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 1.4782 - categorical_accuracy: 0.3431\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 2s 210ms/step - loss: 1.3790 - categorical_accuracy: 0.4121\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 1s 153ms/step - loss: 1.2441 - categorical_accuracy: 0.5657\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 1s 143ms/step - loss: 1.1957 - categorical_accuracy: 0.6348\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 0.9676 - categorical_accuracy: 0.6450\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 1s 149ms/step - loss: 0.8424 - categorical_accuracy: 0.6736\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 1s 144ms/step - loss: 0.7505 - categorical_accuracy: 0.6836\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 0.7511 - categorical_accuracy: 0.6707\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 1s 146ms/step - loss: 1.8549 - categorical_accuracy: 0.5001\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 1s 143ms/step - loss: 1.5450 - categorical_accuracy: 0.3636\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 2s 222ms/step - loss: 1.4982 - categorical_accuracy: 0.5825\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 1s 176ms/step - loss: 1.5827 - categorical_accuracy: 0.2017\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 2s 193ms/step - loss: 1.5966 - categorical_accuracy: 0.2014\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 1s 172ms/step - loss: 1.5822 - categorical_accuracy: 0.2559\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 1s 174ms/step - loss: 1.5525 - categorical_accuracy: 0.3461\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 2s 235ms/step - loss: 1.4453 - categorical_accuracy: 0.3942\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 2s 238ms/step - loss: 1.1523 - categorical_accuracy: 0.3611\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 2s 272ms/step - loss: 20.8062 - categorical_accuracy: 0.3782\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 1.9802 - categorical_accuracy: 0.2286\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 2s 279ms/step - loss: 1.6626 - categorical_accuracy: 0.2425\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 1.5506 - categorical_accuracy: 0.2386\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 3s 399ms/step - loss: 1.4494 - categorical_accuracy: 0.3654\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 4s 465ms/step - loss: 1.2650 - categorical_accuracy: 0.6721\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 2s 240ms/step - loss: 0.8078 - categorical_accuracy: 0.7089\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.6503 - categorical_accuracy: 0.6974\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 2s 306ms/step - loss: 0.6198 - categorical_accuracy: 0.7655\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.5074 - categorical_accuracy: 0.7816\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.4145 - categorical_accuracy: 0.7346\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.4334 - categorical_accuracy: 0.7759\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 0.3869 - categorical_accuracy: 0.8289\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 2s 273ms/step - loss: 0.4153 - categorical_accuracy: 0.7787\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 2s 221ms/step - loss: 0.4121 - categorical_accuracy: 0.7906\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 2s 196ms/step - loss: 0.3387 - categorical_accuracy: 0.7854\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 2s 191ms/step - loss: 0.3674 - categorical_accuracy: 0.8259\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 2s 200ms/step - loss: 0.3291 - categorical_accuracy: 0.7978\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 2s 253ms/step - loss: 0.2838 - categorical_accuracy: 0.8299\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 2s 195ms/step - loss: 0.3108 - categorical_accuracy: 0.8454\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 2s 226ms/step - loss: 0.2872 - categorical_accuracy: 0.8492\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 2s 268ms/step - loss: 0.3043 - categorical_accuracy: 0.8638\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 2s 302ms/step - loss: 1.5147 - categorical_accuracy: 0.5654\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 2s 243ms/step - loss: 0.8740 - categorical_accuracy: 0.6711\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 2s 224ms/step - loss: 0.4158 - categorical_accuracy: 0.7995\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 2s 238ms/step - loss: 0.3495 - categorical_accuracy: 0.7982\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 2s 285ms/step - loss: 0.3298 - categorical_accuracy: 0.8131\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 2s 301ms/step - loss: 0.3013 - categorical_accuracy: 0.8591\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 3s 422ms/step - loss: 0.4912 - categorical_accuracy: 0.7651\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.5729 - categorical_accuracy: 0.7390\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 3s 321ms/step - loss: 0.5006 - categorical_accuracy: 0.7428\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 2s 238ms/step - loss: 0.4778 - categorical_accuracy: 0.7541\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 2s 217ms/step - loss: 0.3888 - categorical_accuracy: 0.7787\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 2s 295ms/step - loss: 0.3779 - categorical_accuracy: 0.7949\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 2s 214ms/step - loss: 0.4385 - categorical_accuracy: 0.7376\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 2s 209ms/step - loss: 0.5176 - categorical_accuracy: 0.8024\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 2s 208ms/step - loss: 0.4123 - categorical_accuracy: 0.7502\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 3s 297ms/step - loss: 0.4411 - categorical_accuracy: 0.7946\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 2s 220ms/step - loss: 0.3205 - categorical_accuracy: 0.8361\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 2s 257ms/step - loss: 0.3383 - categorical_accuracy: 0.8638\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.3271 - categorical_accuracy: 0.8176\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 2s 285ms/step - loss: 0.3186 - categorical_accuracy: 0.8555\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.8500 - categorical_accuracy: 0.7347\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 1.8256 - categorical_accuracy: 0.2477\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 2s 290ms/step - loss: 1.6114 - categorical_accuracy: 0.2280\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 2s 306ms/step - loss: 1.4836 - categorical_accuracy: 0.3357\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 2s 225ms/step - loss: 1.4103 - categorical_accuracy: 0.4212\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 2s 271ms/step - loss: 1.3449 - categorical_accuracy: 0.3970\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 2s 267ms/step - loss: 1.2058 - categorical_accuracy: 0.5817\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 1.1339 - categorical_accuracy: 0.5204\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 1.0560 - categorical_accuracy: 0.5266\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 3s 308ms/step - loss: 1.2586 - categorical_accuracy: 0.4754\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 3s 385ms/step - loss: 1.2859 - categorical_accuracy: 0.4947\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 2s 289ms/step - loss: 1.0225 - categorical_accuracy: 0.5703\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 2s 319ms/step - loss: 0.6823 - categorical_accuracy: 0.6851\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 3s 360ms/step - loss: 0.5605 - categorical_accuracy: 0.7278\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 2s 280ms/step - loss: 0.2954 - categorical_accuracy: 0.8513\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 3s 372ms/step - loss: 0.3974 - categorical_accuracy: 0.7548\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 3s 368ms/step - loss: 0.3677 - categorical_accuracy: 0.8036\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 0.4419 - categorical_accuracy: 0.7542\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 3s 314ms/step - loss: 0.3846 - categorical_accuracy: 0.7881\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 3s 322ms/step - loss: 0.3299 - categorical_accuracy: 0.7934\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.3232 - categorical_accuracy: 0.8097\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 2s 286ms/step - loss: 0.3505 - categorical_accuracy: 0.7802\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 2s 306ms/step - loss: 0.3271 - categorical_accuracy: 0.8390\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 2s 294ms/step - loss: 0.3506 - categorical_accuracy: 0.8028\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 2s 296ms/step - loss: 0.3077 - categorical_accuracy: 0.8330\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 2s 276ms/step - loss: 0.3049 - categorical_accuracy: 0.8088\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 2s 286ms/step - loss: 0.2820 - categorical_accuracy: 0.8216\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 2s 275ms/step - loss: 0.3077 - categorical_accuracy: 0.8190\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 2s 302ms/step - loss: 0.4022 - categorical_accuracy: 0.7642\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 0.3229 - categorical_accuracy: 0.8151\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 2s 268ms/step - loss: 0.3215 - categorical_accuracy: 0.7910\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 3s 335ms/step - loss: 0.3616 - categorical_accuracy: 0.8391\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 3s 368ms/step - loss: 0.2708 - categorical_accuracy: 0.8374\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 3s 351ms/step - loss: 0.2875 - categorical_accuracy: 0.8458\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 0.3638 - categorical_accuracy: 0.8043\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 3s 388ms/step - loss: 0.2792 - categorical_accuracy: 0.8399\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 3s 309ms/step - loss: 0.2697 - categorical_accuracy: 0.8521\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 3s 336ms/step - loss: 0.2479 - categorical_accuracy: 0.8826\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 2s 307ms/step - loss: 0.3802 - categorical_accuracy: 0.8292\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 2s 260ms/step - loss: 0.3138 - categorical_accuracy: 0.8252\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 2s 242ms/step - loss: 0.3593 - categorical_accuracy: 0.7781\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 2s 235ms/step - loss: 0.2817 - categorical_accuracy: 0.8398\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 2s 291ms/step - loss: 0.2773 - categorical_accuracy: 0.8402\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 3s 347ms/step - loss: 0.3082 - categorical_accuracy: 0.8431\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 3s 318ms/step - loss: 0.3067 - categorical_accuracy: 0.8481\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 2s 306ms/step - loss: 0.2690 - categorical_accuracy: 0.8410\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 3s 382ms/step - loss: 0.2469 - categorical_accuracy: 0.8933\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 3s 385ms/step - loss: 0.2470 - categorical_accuracy: 0.9014\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 3s 376ms/step - loss: 0.2546 - categorical_accuracy: 0.8654\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 3s 436ms/step - loss: 0.2280 - categorical_accuracy: 0.8532\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 3s 363ms/step - loss: 0.2317 - categorical_accuracy: 0.8615\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.3775 - categorical_accuracy: 0.7954\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 0.2994 - categorical_accuracy: 0.8037\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.2540 - categorical_accuracy: 0.8112\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 4s 466ms/step - loss: 0.2036 - categorical_accuracy: 0.9122\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 4s 432ms/step - loss: 0.1967 - categorical_accuracy: 0.9046\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 3s 384ms/step - loss: 0.2191 - categorical_accuracy: 0.8947\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 3s 414ms/step - loss: 0.1675 - categorical_accuracy: 0.9200\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 3s 371ms/step - loss: 0.2434 - categorical_accuracy: 0.8923\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 3s 334ms/step - loss: 0.2649 - categorical_accuracy: 0.8553\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 5s 593ms/step - loss: 0.3034 - categorical_accuracy: 0.8375\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 5s 572ms/step - loss: 0.2720 - categorical_accuracy: 0.8374\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 3s 352ms/step - loss: 0.2764 - categorical_accuracy: 0.8621\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 2s 293ms/step - loss: 0.2067 - categorical_accuracy: 0.8944\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 2s 267ms/step - loss: 0.2258 - categorical_accuracy: 0.9036\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.2316 - categorical_accuracy: 0.8915\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 2s 288ms/step - loss: 0.1968 - categorical_accuracy: 0.8895\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 2s 281ms/step - loss: 0.1690 - categorical_accuracy: 0.9048\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 2s 284ms/step - loss: 0.1734 - categorical_accuracy: 0.9117\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 2s 263ms/step - loss: 0.1830 - categorical_accuracy: 0.9016\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 2s 302ms/step - loss: 0.2419 - categorical_accuracy: 0.8744\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 2s 265ms/step - loss: 0.1686 - categorical_accuracy: 0.9068\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 2s 266ms/step - loss: 0.1549 - categorical_accuracy: 0.9119\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 3s 317ms/step - loss: 0.1607 - categorical_accuracy: 0.9105\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 0.1996 - categorical_accuracy: 0.8945\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 5s 612ms/step - loss: 0.1261 - categorical_accuracy: 0.9300\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 5s 540ms/step - loss: 0.1180 - categorical_accuracy: 0.9443\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 4s 553ms/step - loss: 0.1300 - categorical_accuracy: 0.9335\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 4s 536ms/step - loss: 0.1662 - categorical_accuracy: 0.9218\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 4s 485ms/step - loss: 0.1690 - categorical_accuracy: 0.9160\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 4s 490ms/step - loss: 0.1204 - categorical_accuracy: 0.9340\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 2s 292ms/step - loss: 0.1513 - categorical_accuracy: 0.9133\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 3s 342ms/step - loss: 0.3737 - categorical_accuracy: 0.8768\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 2s 311ms/step - loss: 0.2912 - categorical_accuracy: 0.8175\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 3s 338ms/step - loss: 0.2585 - categorical_accuracy: 0.8667\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 3s 337ms/step - loss: 0.2494 - categorical_accuracy: 0.9037\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 4s 514ms/step - loss: 0.3252 - categorical_accuracy: 0.8023\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 4s 473ms/step - loss: 0.2651 - categorical_accuracy: 0.8552\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 5s 612ms/step - loss: 0.2542 - categorical_accuracy: 0.8446\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 5s 611ms/step - loss: 0.2069 - categorical_accuracy: 0.8849\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 5s 644ms/step - loss: 0.2019 - categorical_accuracy: 0.8971\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 5s 580ms/step - loss: 0.1793 - categorical_accuracy: 0.9055\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 5s 646ms/step - loss: 0.1506 - categorical_accuracy: 0.9276\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 6s 700ms/step - loss: 0.1404 - categorical_accuracy: 0.9245\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 5s 504ms/step - loss: 0.1507 - categorical_accuracy: 0.9232\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 3s 307ms/step - loss: 0.1684 - categorical_accuracy: 0.9040\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 3s 333ms/step - loss: 0.1421 - categorical_accuracy: 0.9205\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.1469 - categorical_accuracy: 0.9275\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 0.1861 - categorical_accuracy: 0.9092\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 0.1376 - categorical_accuracy: 0.9329\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 3s 364ms/step - loss: 0.1477 - categorical_accuracy: 0.9322\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 3s 312ms/step - loss: 0.1292 - categorical_accuracy: 0.9573\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 3s 368ms/step - loss: 0.1252 - categorical_accuracy: 0.9357\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 3s 350ms/step - loss: 0.0917 - categorical_accuracy: 0.9421\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 3s 324ms/step - loss: 0.1128 - categorical_accuracy: 0.9298\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 3s 314ms/step - loss: 0.0944 - categorical_accuracy: 0.9614\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 2s 269ms/step - loss: 0.0902 - categorical_accuracy: 0.9544\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 2s 301ms/step - loss: 0.2911 - categorical_accuracy: 0.9128\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 2s 299ms/step - loss: 0.2192 - categorical_accuracy: 0.8552\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 2s 246ms/step - loss: 0.2404 - categorical_accuracy: 0.9015\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 2s 290ms/step - loss: 0.1901 - categorical_accuracy: 0.8827\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 3s 339ms/step - loss: 0.1443 - categorical_accuracy: 0.9356\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 3s 325ms/step - loss: 0.1515 - categorical_accuracy: 0.9171\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 3s 322ms/step - loss: 0.1751 - categorical_accuracy: 0.9540\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 3s 371ms/step - loss: 0.1224 - categorical_accuracy: 0.9429\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 3s 373ms/step - loss: 0.1180 - categorical_accuracy: 0.9544\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 3s 375ms/step - loss: 0.0903 - categorical_accuracy: 0.9835\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 3s 357ms/step - loss: 0.0930 - categorical_accuracy: 0.9710\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 3s 370ms/step - loss: 0.0802 - categorical_accuracy: 0.9667\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 3s 374ms/step - loss: 0.0522 - categorical_accuracy: 0.9855\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 3s 332ms/step - loss: 0.0404 - categorical_accuracy: 0.9820\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 3s 341ms/step - loss: 0.0853 - categorical_accuracy: 0.9657\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 3s 321ms/step - loss: 0.0344 - categorical_accuracy: 0.9886\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 3s 316ms/step - loss: 0.0842 - categorical_accuracy: 0.9691\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 3s 321ms/step - loss: 0.2080 - categorical_accuracy: 0.9159\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 3s 378ms/step - loss: 0.1011 - categorical_accuracy: 0.9531\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 3s 312ms/step - loss: 0.0599 - categorical_accuracy: 0.9740\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 2s 308ms/step - loss: 0.0308 - categorical_accuracy: 0.9950\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 3s 346ms/step - loss: 0.0292 - categorical_accuracy: 0.9953\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 3s 378ms/step - loss: 0.0851 - categorical_accuracy: 0.9696\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 3s 416ms/step - loss: 0.1531 - categorical_accuracy: 0.9517\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 3s 386ms/step - loss: 0.0685 - categorical_accuracy: 0.9811\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 3s 337ms/step - loss: 0.0603 - categorical_accuracy: 0.9723\n",
      "Epoch 201/300\n",
      "3/8 [==========>...................] - ETA: 2s - loss: 0.0374 - categorical_accuracy: 0.9965"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8974e9244892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtb_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=300, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 25, 64)            442112    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 165       \n",
      "=================================================================\n",
      "Total params: 596,741\n",
      "Trainable params: 596,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8. making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goodbye'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(res[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goodbye'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[np.argmax(y_test[2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9. save our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save('sign_language_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading model\n",
    "model.load_weights('sign_language_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10. evaluation to see how this model is performing. import a couple of metrics from scikit learn to evaluate the performance of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_train, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[188,   0],\n",
       "        [  5,  44]],\n",
       "\n",
       "       [[187,   5],\n",
       "        [  0,  45]],\n",
       "\n",
       "       [[189,   0],\n",
       "        [  0,  48]],\n",
       "\n",
       "       [[191,   0],\n",
       "        [  0,  46]],\n",
       "\n",
       "       [[188,   0],\n",
       "        [  0,  49]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789029535864979"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test in Real Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11. perform a real time detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coloring the action strings\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245), (204,204,255), (255, 255, 102)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    #all of our different probabilities\n",
    "    for num, prob in enumerate(res):\n",
    "        #drawing a dynamic rectangle\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        #output text\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "       \n",
    "    #cv2.rectangle(output_frame, (0,60+np.argmax(res)*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Goodbye\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "Goodbye\n",
      "Thanks\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "House\n",
      "House\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "House\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Thanks\n",
      "House\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Thanks\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Nice To Meet You\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Hello\n",
      "Hello\n",
      "House\n",
      "House\n",
      "House\n",
      "House\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n",
      "Goodbye\n"
     ]
    }
   ],
   "source": [
    "#collect our 25 frames in order to be able to generate a prediction\n",
    "sequence = []\n",
    "#allow us to concatenate our history of detections together \n",
    "sentence = []\n",
    "#\n",
    "predictions = []\n",
    "#render results if they're above a certain threshold\n",
    "threshold = 0.8\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "        #read feed\n",
    "        ret, frame = cap.read()\n",
    "        #make predictions\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        #draw formated landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        #prediction logic\n",
    "        #extracting keypoints\n",
    "        keypoints = extract_keypoints(results)\n",
    "        #appending our key points to the end of the sequance\n",
    "        sequence.append(keypoints)\n",
    "        #to grab our last 25 frames to be able to generate our prediction \n",
    "        sequence = sequence[-25:]\n",
    "        #if the length of length of the sequence=25 then and only then will we run a prediction model\n",
    "        if len(sequence) == 25:\n",
    "            #generate prediction model\n",
    "            res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "            predictions.append(np.argmax(res))\n",
    "            \n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "                #visualization logic\n",
    "                #check if our result is above the threshold\n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    #checking whether or not we've more than a certain number of words or whether or not we've got words\n",
    "                    if len(sentence) > 0:\n",
    "                        #checking if the current action does not equal the last sentence in our string to avoid double\n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            #append the current detected action onto our sequatnce array\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        #append the initial action\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "            if len(sentence) > 5: \n",
    "                #grabbing the last five values don't end up with this giant array to render\n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "                \n",
    "            image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        #show the screen\n",
    "        cv2.imshow('Project Feed', image)\n",
    "        #break if ('q') is pressed\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
